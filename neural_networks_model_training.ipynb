{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('auto-mpg.data', delimiter=r'\\s+', header=None)\n",
    "df = df.iloc[:, :-1]\n",
    "df.iloc[df.loc[:, 3] == '?', 3] = np.nan\n",
    "df[3] = df[3].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-mpg\n",
    "si = SimpleImputer(strategy='mean', missing_values=np.nan)\n",
    "df[3] = si.fit_transform(df[[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.iloc[:, :7]\n",
    "df_2 = df.iloc[:, [7]]\n",
    "dataset_1 = df_1.to_numpy()\n",
    "dataset_2 = df_2.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = ohe.fit_transform(dataset_2)\n",
    "dataset = np.concatenate([dataset_1, dataset_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=[7], dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = SimpleImputer(strategy='mean', missing_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_features = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "df[impute_features] = si.fit_transform(df[impute_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for auto-mpg\n",
    "dataset_x = dataset[:, 1:]\n",
    "dataset_y = dataset[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_x = dataset[:, :-1]\n",
    "dataset_y = dataset[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_x, test_dataset_x, training_dataset_y, test_dataset_y = train_test_split(dataset_x, dataset_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-mpg\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(training_dataset_x)\n",
    "scaled_training_dataset_x = ss.transform(training_dataset_x)\n",
    "scaled_test_dataset_x = ss.transform(test_dataset_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        loss = logs['loss']\n",
    "        val_loss = logs['val_loss']\n",
    "        print(f'epoch: {epoch}, loss: {loss}, val_loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, Normalization, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(name='Auto-MPG')\n",
    "\n",
    "model.add(Input((training_dataset_x.shape[1],)))\n",
    "model.add(Dense(32, activation='relu', name='Hidden-1'))\n",
    "model.add(Dense(32, activation='relu', name='Hidden-2'))\n",
    "model.add(Dense(1, activation='linear', name='Output'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-mpg\n",
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "hist = model.fit(scaled_training_dataset_x, training_dataset_y, batch_size=32, epochs=200, validation_split=0.2)\n",
    "eval_result = model.evaluate(scaled_test_dataset_x, test_dataset_y, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mins = np.min(training_dataset_x, axis=0)\n",
    "maxmin_diffs = np.max(training_dataset_x, axis=0) - np.min(training_dataset_x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_layer = Normalization()  # Standard Scalar\n",
    "norm_layer.adapt(training_dataset_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_layer = Normalization(mean=mins, variance=maxmin_diffs ** 2)   # minmax scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(name='Diabetes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Input((training_dataset_x.shape[1],)))\n",
    "model.add(norm_layer)\n",
    "model.add(Dense(16, activation='relu', name='Hidden-1'))\n",
    "model.add(Dense(16, activation='relu', name='Hidden-2'))\n",
    "model.add(Dense(1, activation='sigmoid', name='Output'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2)\n",
    "eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycallback = MyCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=300, validation_split=0.2, callbacks=[mycallback], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end_proc(epoch, logs):\n",
    "    pass\n",
    "def on_batch_begin_proc(batch, logs):\n",
    "    pass\n",
    "def on_batch_end_proc(batch, logs):\n",
    "    pass\n",
    "\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "\n",
    "lambda_callback = LambdaCallback(on_epoch_end=on_epoch_end_proc, on_batch_begin=on_batch_begin_proc, on_batch_end=on_batch_end_proc)\n",
    "hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=300, validation_split=0.2, callbacks=[lambda_callback], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLambdaCallback(Callback):\n",
    "    def __init__(self, on_epoch_begin=None, on_epoch_end=None, on_batch_begin=None,  on_batch_end=None):\n",
    "        self._on_epoch_begin = on_epoch_begin\n",
    "        self._on_epoch_end = on_epoch_end\n",
    "        self._on_batch_begin = on_batch_begin\n",
    "        self._on_batch_end = on_batch_end\n",
    "        \n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        if self._on_epoch_begin:\n",
    "            self._on_epoch_begin(epoch, logs)\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if self._on_epoch_end:\n",
    "            self._on_epoch_end(epoch, logs)\n",
    "    \n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self._on_batch_begin:\n",
    "            self._on_batch_begin(batch, logs)\n",
    "                \n",
    "    def on_batch_end(self, batch, logs):\n",
    "        if self._on_batch_end:\n",
    "            self._on_batch_end(batch, logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_losses= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_begin_proc(epoch, logs):\n",
    "    global batch_losses\n",
    "    batch_losses = []\n",
    "    print(f'eopch: {epoch}')    \n",
    "\n",
    "\n",
    "def on_epoch_end_proc(epoch, logs):\n",
    "    loss = logs['loss']\n",
    "    val_loss = logs['val_loss']\n",
    "    print(f'\\nepoch: {epoch}, loss: {loss}, val_loss: {val_loss}')\n",
    "    print(f'batch mean: {np.mean(batch_losses)}')\n",
    "    print('-' * 30)\n",
    "  \n",
    "def on_batch_end_proc(batch, logs):\n",
    "    global total\n",
    "    loss = logs['loss']\n",
    "    batch_losses.append(loss)\n",
    "    print(f'\\t\\tbatch: {batch}, loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylambda_callback = MyLambdaCallback(on_epoch_begin=on_epoch_begin_proc, on_epoch_end=on_epoch_end_proc, on_batch_end=on_batch_end_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(training_dataset_x, training_dataset_y, batch_size=32, epochs=100, validation_split=0.2, callbacks=[mylambda_callback], verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "plt.title('Epoch - Loss Graph', pad=10, fontsize=14)\n",
    "plt.xticks(range(0, 300, 10))\n",
    "plt.plot(hist.epoch, hist.history['loss'])\n",
    "plt.plot(hist.epoch, hist.history['val_loss'])\n",
    "plt.legend(['Loss', 'Validation Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-mpg\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.title('Epoch - Mean Absolute Error', pad=10, fontsize=14)\n",
    "plt.xticks(range(0, 300, 10))\n",
    "plt.plot(hist.epoch, hist.history['mae'])\n",
    "plt.plot(hist.epoch, hist.history['val_mae'])\n",
    "plt.legend(['Measn Absolute Error', 'Validation Mean Absolute Error'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('auto-mpg.h5')\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('auto-mpg.pickle', 'wb') as f:\n",
    "    pickle.dump((ohe, ss), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('auto-mpg.h5')\n",
    "\n",
    "with open('auto-mpg.pickle', 'rb') as f:\n",
    "    ohe, ss = pickle.load(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-mpg prediction one hot encoder\n",
    "predict_df = pd.read_csv('predict.csv', header=None)\n",
    "predict_df_1 = predict_df.iloc[:, :6]\n",
    "predict_df_2 = predict_df.iloc[:, [6]]\n",
    "\n",
    "predict_dataset_1 = predict_df_1.to_numpy()\n",
    "predict_dataset_2 = predict_df_2.to_numpy()\n",
    "predict_dataset_2  = ohe.transform(predict_dataset_2)\n",
    "\n",
    "predict_dataset = np.concatenate([predict_dataset_1, predict_dataset_2], axis=1)\n",
    "scaled_predict_dataset = ss.transform(predict_dataset)\n",
    "predict_result = model.predict(scaled_predict_dataset)\n",
    "\n",
    "for val in predict_result[:, 0]:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-mpg prediction\n",
    "predict_df = pd.read_csv('predict.csv', header=None)\n",
    "predict_df = pd.get_dummies(predict_df, columns=[6])\n",
    "predict_dataset_x = predict_df.to_numpy() \n",
    "scaled_predict_dataset_x = ss.transform(predict_dataset_x)\n",
    "\n",
    "predict_result = model.predict(scaled_predict_dataset_x)\n",
    "\n",
    "for val in predict_result[:, 0]:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "plt.title('Epoch - Binary Accuracy Graph', pad=10, fontsize=14)\n",
    "plt.xticks(range(0, 300, 10))\n",
    "plt.plot(hist.epoch, hist.history['binary_accuracy'])\n",
    "plt.plot(hist.epoch, hist.history['val_binary_accuracy'])\n",
    "plt.legend(['Accuracy', 'Validation Accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = model.evaluate(test_dataset_x, test_dataset_y, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(eval_result)):\n",
    "    print(f'{model.metrics_names[i]}: {eval_result[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataset = np.array([[2 ,90, 68, 12, 120, 38.2, 0.503, 28],\n",
    "                            [4, 111, 79, 47, 207, 37.1, 1.39, 56],\n",
    "                            [3, 190, 65, 25, 130, 34, 0.271, 26],\n",
    "                            [8, 176, 90, 34, 300, 50.7, 0.467, 58],\n",
    "                            [7, 106, 92, 18, 200, 35, 0.300, 48]])\n",
    "\n",
    "model.save('diabetes.h5')\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('diabetes.h5')\n",
    "\n",
    "predict_result = model.predict(predict_dataset)\n",
    "print(predict_result)\n",
    "\n",
    "for result in predict_result[:, 0]:\n",
    "    print('Şeker hastasi' if result > 0.5 else 'Şeker Hastasi Değil')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
